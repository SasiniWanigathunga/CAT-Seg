[08/24 15:41:23] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 15:41:23] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 15:41:23] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/ade150.json', 'DATASETS.TEST', '("ade20k_150_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_lage.pth'])
[08/24 15:41:23] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 15:41:23] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ade20k_150_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/ade150.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_lage.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 15:41:23] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 15:41:23] d2.utils.env INFO: Using a generated random seed 23898319
[08/24 15:42:16] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 15:42:16] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:19] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 15:42:19] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 15:42:19] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/ade847.json', 'DATASETS.TEST', '("ade20k_full_sem_seg_freq_val_all",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_lage.pth'])
[08/24 15:42:19] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 15:42:19] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ade20k_full_sem_seg_freq_val_all
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/ade847.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_lage.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 15:42:19] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 15:42:19] d2.utils.env INFO: Using a generated random seed 19723509
[08/24 15:42:27] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 15:42:27] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:27] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:29] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 15:42:30] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 15:42:30] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/voc20.json', 'DATASETS.TEST', '("voc_2012_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_lage.pth'])
[08/24 15:42:30] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 15:42:30] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - voc_2012_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/voc20.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_lage.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 15:42:30] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 15:42:30] d2.utils.env INFO: Using a generated random seed 30558165
[08/24 15:42:35] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 15:42:35] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:35] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:37] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 15:42:37] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 15:42:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/voc20b.json', 'DATASETS.TEST', '("voc_2012_test_background_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_lage.pth'])
[08/24 15:42:37] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 15:42:37] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - voc_2012_test_background_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/voc20b.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_lage.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 15:42:37] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 15:42:37] d2.utils.env INFO: Using a generated random seed 38040441
[08/24 15:42:42] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 15:42:42] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:45] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 15:42:45] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 15:42:45] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/pc59.json', 'DATASETS.TEST', '("context_59_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_lage.pth'])
[08/24 15:42:45] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 15:42:45] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - context_59_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/pc59.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_lage.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 15:42:45] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 15:42:45] d2.utils.env INFO: Using a generated random seed 45676284
[08/24 15:42:50] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 15:42:50] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:52] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 15:42:53] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 15:42:53] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/pc459.json', 'DATASETS.TEST', '("context_459_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_lage.pth'])
[08/24 15:42:53] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 15:42:53] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - context_459_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/pc459.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_lage.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 15:42:53] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 15:42:53] d2.utils.env INFO: Using a generated random seed 53335489
[08/24 15:42:59] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 15:42:59] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:42:59] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_lage.pth ...
[08/24 15:43:21] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 15:43:21] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 15:43:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/ade150.json', 'DATASETS.TEST', '("ade20k_150_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 15:43:21] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 15:43:21] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ade20k_150_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/ade150.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 15:43:21] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 15:43:21] d2.utils.env INFO: Using a generated random seed 22207646
[08/24 15:43:27] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 15:43:27] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 15:43:27] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 15:43:28] d2.data.datasets.coco INFO: Loaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation
[08/24 15:43:28] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]
[08/24 15:43:28] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[08/24 15:43:28] d2.data.common INFO: Serializing 2000 elements to byte tensors and concatenating them all ...
[08/24 15:43:28] d2.data.common INFO: Serialized dataset takes 0.39 MiB
[08/24 15:43:28] d2.data.datasets.coco INFO: Loaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation
[08/24 15:43:28] d2.evaluation.evaluator INFO: Start inference on 2000 batches
[08/24 15:43:38] d2.evaluation.evaluator INFO: Inference done 11/2000. Dataloading: 0.0007 s/iter. Inference: 0.8109 s/iter. Eval: 0.0070 s/iter. Total: 0.8186 s/iter. ETA=0:27:08
[08/24 15:43:43] d2.evaluation.evaluator INFO: Inference done 18/2000. Dataloading: 0.0010 s/iter. Inference: 0.8111 s/iter. Eval: 0.0068 s/iter. Total: 0.8190 s/iter. ETA=0:27:03
[08/24 15:43:49] d2.evaluation.evaluator INFO: Inference done 25/2000. Dataloading: 0.0011 s/iter. Inference: 0.8113 s/iter. Eval: 0.0062 s/iter. Total: 0.8187 s/iter. ETA=0:26:56
[08/24 15:43:55] d2.evaluation.evaluator INFO: Inference done 32/2000. Dataloading: 0.0012 s/iter. Inference: 0.8114 s/iter. Eval: 0.0065 s/iter. Total: 0.8191 s/iter. ETA=0:26:52
[08/24 15:44:01] d2.evaluation.evaluator INFO: Inference done 39/2000. Dataloading: 0.0012 s/iter. Inference: 0.8115 s/iter. Eval: 0.0062 s/iter. Total: 0.8190 s/iter. ETA=0:26:45
[08/24 15:44:06] d2.evaluation.evaluator INFO: Inference done 46/2000. Dataloading: 0.0013 s/iter. Inference: 0.8116 s/iter. Eval: 0.0064 s/iter. Total: 0.8193 s/iter. ETA=0:26:40
[08/24 15:44:12] d2.evaluation.evaluator INFO: Inference done 53/2000. Dataloading: 0.0013 s/iter. Inference: 0.8118 s/iter. Eval: 0.0067 s/iter. Total: 0.8199 s/iter. ETA=0:26:36
[08/24 15:44:18] d2.evaluation.evaluator INFO: Inference done 60/2000. Dataloading: 0.0013 s/iter. Inference: 0.8121 s/iter. Eval: 0.0069 s/iter. Total: 0.8203 s/iter. ETA=0:26:31
[08/24 15:44:24] d2.evaluation.evaluator INFO: Inference done 67/2000. Dataloading: 0.0014 s/iter. Inference: 0.8123 s/iter. Eval: 0.0066 s/iter. Total: 0.8203 s/iter. ETA=0:26:25
[08/24 15:44:29] d2.evaluation.evaluator INFO: Inference done 74/2000. Dataloading: 0.0014 s/iter. Inference: 0.8124 s/iter. Eval: 0.0065 s/iter. Total: 0.8203 s/iter. ETA=0:26:19
[08/24 15:44:35] d2.evaluation.evaluator INFO: Inference done 81/2000. Dataloading: 0.0014 s/iter. Inference: 0.8126 s/iter. Eval: 0.0067 s/iter. Total: 0.8208 s/iter. ETA=0:26:15
[08/24 15:44:41] d2.evaluation.evaluator INFO: Inference done 88/2000. Dataloading: 0.0014 s/iter. Inference: 0.8128 s/iter. Eval: 0.0066 s/iter. Total: 0.8208 s/iter. ETA=0:26:09
[08/24 15:44:47] d2.evaluation.evaluator INFO: Inference done 95/2000. Dataloading: 0.0014 s/iter. Inference: 0.8130 s/iter. Eval: 0.0068 s/iter. Total: 0.8212 s/iter. ETA=0:26:04
[08/24 15:44:52] d2.evaluation.evaluator INFO: Inference done 102/2000. Dataloading: 0.0014 s/iter. Inference: 0.8131 s/iter. Eval: 0.0071 s/iter. Total: 0.8217 s/iter. ETA=0:25:59
[08/24 15:44:58] d2.evaluation.evaluator INFO: Inference done 109/2000. Dataloading: 0.0014 s/iter. Inference: 0.8133 s/iter. Eval: 0.0072 s/iter. Total: 0.8219 s/iter. ETA=0:25:54
[08/24 15:45:04] d2.evaluation.evaluator INFO: Inference done 116/2000. Dataloading: 0.0014 s/iter. Inference: 0.8134 s/iter. Eval: 0.0070 s/iter. Total: 0.8218 s/iter. ETA=0:25:48
[08/24 15:45:10] d2.evaluation.evaluator INFO: Inference done 123/2000. Dataloading: 0.0014 s/iter. Inference: 0.8135 s/iter. Eval: 0.0072 s/iter. Total: 0.8221 s/iter. ETA=0:25:43
[08/24 15:45:16] d2.evaluation.evaluator INFO: Inference done 130/2000. Dataloading: 0.0014 s/iter. Inference: 0.8137 s/iter. Eval: 0.0074 s/iter. Total: 0.8225 s/iter. ETA=0:25:38
[08/24 15:45:21] d2.evaluation.evaluator INFO: Inference done 137/2000. Dataloading: 0.0014 s/iter. Inference: 0.8138 s/iter. Eval: 0.0074 s/iter. Total: 0.8227 s/iter. ETA=0:25:32
[08/24 15:45:27] d2.evaluation.evaluator INFO: Inference done 144/2000. Dataloading: 0.0014 s/iter. Inference: 0.8140 s/iter. Eval: 0.0076 s/iter. Total: 0.8230 s/iter. ETA=0:25:27
[08/24 15:45:33] d2.evaluation.evaluator INFO: Inference done 151/2000. Dataloading: 0.0014 s/iter. Inference: 0.8141 s/iter. Eval: 0.0076 s/iter. Total: 0.8232 s/iter. ETA=0:25:22
[08/24 15:45:39] d2.evaluation.evaluator INFO: Inference done 158/2000. Dataloading: 0.0014 s/iter. Inference: 0.8142 s/iter. Eval: 0.0077 s/iter. Total: 0.8234 s/iter. ETA=0:25:16
[08/24 15:45:44] d2.evaluation.evaluator INFO: Inference done 165/2000. Dataloading: 0.0014 s/iter. Inference: 0.8143 s/iter. Eval: 0.0077 s/iter. Total: 0.8235 s/iter. ETA=0:25:11
[08/24 15:45:50] d2.evaluation.evaluator INFO: Inference done 172/2000. Dataloading: 0.0014 s/iter. Inference: 0.8144 s/iter. Eval: 0.0078 s/iter. Total: 0.8237 s/iter. ETA=0:25:05
[08/24 15:45:56] d2.evaluation.evaluator INFO: Inference done 179/2000. Dataloading: 0.0014 s/iter. Inference: 0.8145 s/iter. Eval: 0.0079 s/iter. Total: 0.8238 s/iter. ETA=0:25:00
[08/24 15:46:02] d2.evaluation.evaluator INFO: Inference done 186/2000. Dataloading: 0.0014 s/iter. Inference: 0.8146 s/iter. Eval: 0.0079 s/iter. Total: 0.8240 s/iter. ETA=0:24:54
[08/24 15:46:08] d2.evaluation.evaluator INFO: Inference done 193/2000. Dataloading: 0.0014 s/iter. Inference: 0.8146 s/iter. Eval: 0.0078 s/iter. Total: 0.8239 s/iter. ETA=0:24:48
[08/24 15:46:13] d2.evaluation.evaluator INFO: Inference done 200/2000. Dataloading: 0.0014 s/iter. Inference: 0.8147 s/iter. Eval: 0.0077 s/iter. Total: 0.8239 s/iter. ETA=0:24:43
[08/24 15:46:19] d2.evaluation.evaluator INFO: Inference done 207/2000. Dataloading: 0.0014 s/iter. Inference: 0.8148 s/iter. Eval: 0.0076 s/iter. Total: 0.8239 s/iter. ETA=0:24:37
[08/24 15:46:25] d2.evaluation.evaluator INFO: Inference done 214/2000. Dataloading: 0.0014 s/iter. Inference: 0.8148 s/iter. Eval: 0.0074 s/iter. Total: 0.8237 s/iter. ETA=0:24:31
[08/24 15:46:31] d2.evaluation.evaluator INFO: Inference done 221/2000. Dataloading: 0.0014 s/iter. Inference: 0.8149 s/iter. Eval: 0.0074 s/iter. Total: 0.8237 s/iter. ETA=0:24:25
[08/24 15:46:36] d2.evaluation.evaluator INFO: Inference done 228/2000. Dataloading: 0.0014 s/iter. Inference: 0.8150 s/iter. Eval: 0.0073 s/iter. Total: 0.8237 s/iter. ETA=0:24:19
[08/24 15:46:42] d2.evaluation.evaluator INFO: Inference done 235/2000. Dataloading: 0.0014 s/iter. Inference: 0.8150 s/iter. Eval: 0.0073 s/iter. Total: 0.8238 s/iter. ETA=0:24:14
[08/24 15:46:48] d2.evaluation.evaluator INFO: Inference done 242/2000. Dataloading: 0.0014 s/iter. Inference: 0.8151 s/iter. Eval: 0.0073 s/iter. Total: 0.8239 s/iter. ETA=0:24:08
[08/24 15:46:54] d2.evaluation.evaluator INFO: Inference done 249/2000. Dataloading: 0.0014 s/iter. Inference: 0.8151 s/iter. Eval: 0.0073 s/iter. Total: 0.8239 s/iter. ETA=0:24:02
[08/24 15:46:59] d2.evaluation.evaluator INFO: Inference done 256/2000. Dataloading: 0.0014 s/iter. Inference: 0.8152 s/iter. Eval: 0.0073 s/iter. Total: 0.8239 s/iter. ETA=0:23:56
[08/24 15:47:05] d2.evaluation.evaluator INFO: Inference done 263/2000. Dataloading: 0.0014 s/iter. Inference: 0.8152 s/iter. Eval: 0.0072 s/iter. Total: 0.8239 s/iter. ETA=0:23:51
[08/24 15:47:11] d2.evaluation.evaluator INFO: Inference done 270/2000. Dataloading: 0.0014 s/iter. Inference: 0.8153 s/iter. Eval: 0.0072 s/iter. Total: 0.8240 s/iter. ETA=0:23:45
[08/24 15:47:17] d2.evaluation.evaluator INFO: Inference done 277/2000. Dataloading: 0.0014 s/iter. Inference: 0.8153 s/iter. Eval: 0.0073 s/iter. Total: 0.8241 s/iter. ETA=0:23:39
[08/24 15:47:23] d2.evaluation.evaluator INFO: Inference done 284/2000. Dataloading: 0.0014 s/iter. Inference: 0.8154 s/iter. Eval: 0.0073 s/iter. Total: 0.8241 s/iter. ETA=0:23:34
[08/24 15:47:28] d2.evaluation.evaluator INFO: Inference done 291/2000. Dataloading: 0.0014 s/iter. Inference: 0.8154 s/iter. Eval: 0.0073 s/iter. Total: 0.8242 s/iter. ETA=0:23:28
[08/24 15:47:34] d2.evaluation.evaluator INFO: Inference done 298/2000. Dataloading: 0.0014 s/iter. Inference: 0.8154 s/iter. Eval: 0.0074 s/iter. Total: 0.8243 s/iter. ETA=0:23:22
[08/24 15:47:40] d2.evaluation.evaluator INFO: Inference done 305/2000. Dataloading: 0.0014 s/iter. Inference: 0.8155 s/iter. Eval: 0.0074 s/iter. Total: 0.8244 s/iter. ETA=0:23:17
[08/24 15:47:46] d2.evaluation.evaluator INFO: Inference done 312/2000. Dataloading: 0.0014 s/iter. Inference: 0.8155 s/iter. Eval: 0.0075 s/iter. Total: 0.8245 s/iter. ETA=0:23:11
[08/24 15:47:52] d2.evaluation.evaluator INFO: Inference done 319/2000. Dataloading: 0.0014 s/iter. Inference: 0.8156 s/iter. Eval: 0.0076 s/iter. Total: 0.8246 s/iter. ETA=0:23:06
[08/24 15:47:57] d2.evaluation.evaluator INFO: Inference done 326/2000. Dataloading: 0.0014 s/iter. Inference: 0.8156 s/iter. Eval: 0.0076 s/iter. Total: 0.8247 s/iter. ETA=0:23:00
[08/24 15:48:03] d2.evaluation.evaluator INFO: Inference done 333/2000. Dataloading: 0.0014 s/iter. Inference: 0.8156 s/iter. Eval: 0.0076 s/iter. Total: 0.8247 s/iter. ETA=0:22:54
[08/24 15:48:09] d2.evaluation.evaluator INFO: Inference done 340/2000. Dataloading: 0.0014 s/iter. Inference: 0.8157 s/iter. Eval: 0.0076 s/iter. Total: 0.8247 s/iter. ETA=0:22:48
[08/24 15:48:15] d2.evaluation.evaluator INFO: Inference done 347/2000. Dataloading: 0.0014 s/iter. Inference: 0.8157 s/iter. Eval: 0.0075 s/iter. Total: 0.8247 s/iter. ETA=0:22:43
[08/24 15:48:21] d2.evaluation.evaluator INFO: Inference done 354/2000. Dataloading: 0.0014 s/iter. Inference: 0.8157 s/iter. Eval: 0.0075 s/iter. Total: 0.8247 s/iter. ETA=0:22:37
[08/24 15:48:26] d2.evaluation.evaluator INFO: Inference done 361/2000. Dataloading: 0.0014 s/iter. Inference: 0.8157 s/iter. Eval: 0.0075 s/iter. Total: 0.8247 s/iter. ETA=0:22:31
[08/24 15:48:32] d2.evaluation.evaluator INFO: Inference done 368/2000. Dataloading: 0.0014 s/iter. Inference: 0.8158 s/iter. Eval: 0.0074 s/iter. Total: 0.8246 s/iter. ETA=0:22:25
[08/24 15:48:38] d2.evaluation.evaluator INFO: Inference done 375/2000. Dataloading: 0.0014 s/iter. Inference: 0.8158 s/iter. Eval: 0.0074 s/iter. Total: 0.8247 s/iter. ETA=0:22:20
[08/24 15:48:44] d2.evaluation.evaluator INFO: Inference done 382/2000. Dataloading: 0.0014 s/iter. Inference: 0.8158 s/iter. Eval: 0.0074 s/iter. Total: 0.8247 s/iter. ETA=0:22:14
[08/24 15:48:49] d2.evaluation.evaluator INFO: Inference done 389/2000. Dataloading: 0.0014 s/iter. Inference: 0.8158 s/iter. Eval: 0.0075 s/iter. Total: 0.8248 s/iter. ETA=0:22:08
[08/24 15:48:55] d2.evaluation.evaluator INFO: Inference done 396/2000. Dataloading: 0.0014 s/iter. Inference: 0.8159 s/iter. Eval: 0.0074 s/iter. Total: 0.8248 s/iter. ETA=0:22:02
[08/24 15:49:01] d2.evaluation.evaluator INFO: Inference done 403/2000. Dataloading: 0.0014 s/iter. Inference: 0.8159 s/iter. Eval: 0.0074 s/iter. Total: 0.8247 s/iter. ETA=0:21:57
[08/24 15:49:07] d2.evaluation.evaluator INFO: Inference done 410/2000. Dataloading: 0.0014 s/iter. Inference: 0.8159 s/iter. Eval: 0.0073 s/iter. Total: 0.8247 s/iter. ETA=0:21:51
[08/24 15:49:12] d2.evaluation.evaluator INFO: Inference done 417/2000. Dataloading: 0.0014 s/iter. Inference: 0.8159 s/iter. Eval: 0.0072 s/iter. Total: 0.8246 s/iter. ETA=0:21:45
[08/24 15:49:18] d2.evaluation.evaluator INFO: Inference done 424/2000. Dataloading: 0.0014 s/iter. Inference: 0.8159 s/iter. Eval: 0.0073 s/iter. Total: 0.8247 s/iter. ETA=0:21:39
[08/24 15:49:24] d2.evaluation.evaluator INFO: Inference done 431/2000. Dataloading: 0.0014 s/iter. Inference: 0.8159 s/iter. Eval: 0.0073 s/iter. Total: 0.8247 s/iter. ETA=0:21:33
[08/24 15:49:30] d2.evaluation.evaluator INFO: Inference done 438/2000. Dataloading: 0.0014 s/iter. Inference: 0.8160 s/iter. Eval: 0.0073 s/iter. Total: 0.8247 s/iter. ETA=0:21:28
[08/24 15:49:36] d2.evaluation.evaluator INFO: Inference done 445/2000. Dataloading: 0.0014 s/iter. Inference: 0.8160 s/iter. Eval: 0.0073 s/iter. Total: 0.8247 s/iter. ETA=0:21:22
[08/24 15:49:41] d2.evaluation.evaluator INFO: Inference done 452/2000. Dataloading: 0.0014 s/iter. Inference: 0.8160 s/iter. Eval: 0.0072 s/iter. Total: 0.8247 s/iter. ETA=0:21:16
[08/24 15:49:47] d2.evaluation.evaluator INFO: Inference done 459/2000. Dataloading: 0.0014 s/iter. Inference: 0.8160 s/iter. Eval: 0.0072 s/iter. Total: 0.8247 s/iter. ETA=0:21:10
[08/24 15:49:53] d2.evaluation.evaluator INFO: Inference done 466/2000. Dataloading: 0.0014 s/iter. Inference: 0.8160 s/iter. Eval: 0.0073 s/iter. Total: 0.8248 s/iter. ETA=0:21:05
[08/24 15:49:59] d2.evaluation.evaluator INFO: Inference done 473/2000. Dataloading: 0.0014 s/iter. Inference: 0.8160 s/iter. Eval: 0.0074 s/iter. Total: 0.8249 s/iter. ETA=0:20:59
[08/24 15:50:05] d2.evaluation.evaluator INFO: Inference done 480/2000. Dataloading: 0.0014 s/iter. Inference: 0.8161 s/iter. Eval: 0.0074 s/iter. Total: 0.8250 s/iter. ETA=0:20:53
[08/24 15:50:10] d2.evaluation.evaluator INFO: Inference done 487/2000. Dataloading: 0.0014 s/iter. Inference: 0.8161 s/iter. Eval: 0.0074 s/iter. Total: 0.8250 s/iter. ETA=0:20:48
[08/24 15:50:16] d2.evaluation.evaluator INFO: Inference done 494/2000. Dataloading: 0.0014 s/iter. Inference: 0.8161 s/iter. Eval: 0.0073 s/iter. Total: 0.8249 s/iter. ETA=0:20:42
[08/24 15:50:22] d2.evaluation.evaluator INFO: Inference done 501/2000. Dataloading: 0.0014 s/iter. Inference: 0.8161 s/iter. Eval: 0.0074 s/iter. Total: 0.8250 s/iter. ETA=0:20:36
[08/24 15:50:28] d2.evaluation.evaluator INFO: Inference done 508/2000. Dataloading: 0.0014 s/iter. Inference: 0.8161 s/iter. Eval: 0.0074 s/iter. Total: 0.8250 s/iter. ETA=0:20:30
[08/24 15:50:33] d2.evaluation.evaluator INFO: Inference done 515/2000. Dataloading: 0.0014 s/iter. Inference: 0.8162 s/iter. Eval: 0.0074 s/iter. Total: 0.8251 s/iter. ETA=0:20:25
[08/24 15:50:39] d2.evaluation.evaluator INFO: Inference done 522/2000. Dataloading: 0.0014 s/iter. Inference: 0.8162 s/iter. Eval: 0.0075 s/iter. Total: 0.8252 s/iter. ETA=0:20:19
[08/24 15:50:45] d2.evaluation.evaluator INFO: Inference done 529/2000. Dataloading: 0.0014 s/iter. Inference: 0.8162 s/iter. Eval: 0.0076 s/iter. Total: 0.8253 s/iter. ETA=0:20:13
[08/24 15:50:51] d2.evaluation.evaluator INFO: Inference done 536/2000. Dataloading: 0.0014 s/iter. Inference: 0.8162 s/iter. Eval: 0.0076 s/iter. Total: 0.8253 s/iter. ETA=0:20:08
[08/24 15:50:57] d2.evaluation.evaluator INFO: Inference done 543/2000. Dataloading: 0.0014 s/iter. Inference: 0.8162 s/iter. Eval: 0.0076 s/iter. Total: 0.8253 s/iter. ETA=0:20:02
[08/24 15:51:02] d2.evaluation.evaluator INFO: Inference done 550/2000. Dataloading: 0.0014 s/iter. Inference: 0.8162 s/iter. Eval: 0.0076 s/iter. Total: 0.8253 s/iter. ETA=0:19:56
[08/24 15:51:08] d2.evaluation.evaluator INFO: Inference done 557/2000. Dataloading: 0.0014 s/iter. Inference: 0.8163 s/iter. Eval: 0.0076 s/iter. Total: 0.8253 s/iter. ETA=0:19:50
[08/24 15:51:14] d2.evaluation.evaluator INFO: Inference done 564/2000. Dataloading: 0.0014 s/iter. Inference: 0.8163 s/iter. Eval: 0.0075 s/iter. Total: 0.8253 s/iter. ETA=0:19:45
[08/24 15:51:20] d2.evaluation.evaluator INFO: Inference done 571/2000. Dataloading: 0.0015 s/iter. Inference: 0.8163 s/iter. Eval: 0.0075 s/iter. Total: 0.8253 s/iter. ETA=0:19:39
[08/24 15:51:26] d2.evaluation.evaluator INFO: Inference done 578/2000. Dataloading: 0.0015 s/iter. Inference: 0.8163 s/iter. Eval: 0.0076 s/iter. Total: 0.8254 s/iter. ETA=0:19:33
[08/24 15:51:31] d2.evaluation.evaluator INFO: Inference done 585/2000. Dataloading: 0.0015 s/iter. Inference: 0.8163 s/iter. Eval: 0.0076 s/iter. Total: 0.8254 s/iter. ETA=0:19:27
[08/24 15:51:37] d2.evaluation.evaluator INFO: Inference done 592/2000. Dataloading: 0.0015 s/iter. Inference: 0.8163 s/iter. Eval: 0.0076 s/iter. Total: 0.8255 s/iter. ETA=0:19:22
[08/24 15:51:43] d2.evaluation.evaluator INFO: Inference done 599/2000. Dataloading: 0.0015 s/iter. Inference: 0.8163 s/iter. Eval: 0.0077 s/iter. Total: 0.8255 s/iter. ETA=0:19:16
[08/24 15:51:49] d2.evaluation.evaluator INFO: Inference done 606/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0076 s/iter. Total: 0.8255 s/iter. ETA=0:19:10
[08/24 15:51:55] d2.evaluation.evaluator INFO: Inference done 613/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0076 s/iter. Total: 0.8255 s/iter. ETA=0:19:04
[08/24 15:52:00] d2.evaluation.evaluator INFO: Inference done 620/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0076 s/iter. Total: 0.8255 s/iter. ETA=0:18:59
[08/24 15:52:06] d2.evaluation.evaluator INFO: Inference done 627/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0076 s/iter. Total: 0.8254 s/iter. ETA=0:18:53
[08/24 15:52:12] d2.evaluation.evaluator INFO: Inference done 634/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0075 s/iter. Total: 0.8254 s/iter. ETA=0:18:47
[08/24 15:52:18] d2.evaluation.evaluator INFO: Inference done 641/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0075 s/iter. Total: 0.8254 s/iter. ETA=0:18:41
[08/24 15:52:23] d2.evaluation.evaluator INFO: Inference done 648/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0075 s/iter. Total: 0.8254 s/iter. ETA=0:18:35
[08/24 15:52:29] d2.evaluation.evaluator INFO: Inference done 655/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0075 s/iter. Total: 0.8254 s/iter. ETA=0:18:30
[08/24 15:52:35] d2.evaluation.evaluator INFO: Inference done 662/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0075 s/iter. Total: 0.8254 s/iter. ETA=0:18:24
[08/24 15:52:41] d2.evaluation.evaluator INFO: Inference done 669/2000. Dataloading: 0.0015 s/iter. Inference: 0.8164 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:18:18
[08/24 15:52:47] d2.evaluation.evaluator INFO: Inference done 676/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:18:12
[08/24 15:52:52] d2.evaluation.evaluator INFO: Inference done 683/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:18:07
[08/24 15:52:58] d2.evaluation.evaluator INFO: Inference done 690/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:18:01
[08/24 15:53:04] d2.evaluation.evaluator INFO: Inference done 697/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:17:55
[08/24 15:53:10] d2.evaluation.evaluator INFO: Inference done 704/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:17:49
[08/24 15:53:15] d2.evaluation.evaluator INFO: Inference done 711/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:17:43
[08/24 15:53:21] d2.evaluation.evaluator INFO: Inference done 718/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:17:38
[08/24 15:53:27] d2.evaluation.evaluator INFO: Inference done 725/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0074 s/iter. Total: 0.8254 s/iter. ETA=0:17:32
[08/24 15:53:33] d2.evaluation.evaluator INFO: Inference done 732/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0073 s/iter. Total: 0.8254 s/iter. ETA=0:17:26
[08/24 15:53:38] d2.evaluation.evaluator INFO: Inference done 739/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0073 s/iter. Total: 0.8253 s/iter. ETA=0:17:20
[08/24 15:53:44] d2.evaluation.evaluator INFO: Inference done 746/2000. Dataloading: 0.0015 s/iter. Inference: 0.8165 s/iter. Eval: 0.0073 s/iter. Total: 0.8253 s/iter. ETA=0:17:14
[08/24 15:53:50] d2.evaluation.evaluator INFO: Inference done 753/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0073 s/iter. Total: 0.8253 s/iter. ETA=0:17:09
[08/24 15:53:56] d2.evaluation.evaluator INFO: Inference done 760/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0072 s/iter. Total: 0.8253 s/iter. ETA=0:17:03
[08/24 15:54:02] d2.evaluation.evaluator INFO: Inference done 767/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0072 s/iter. Total: 0.8253 s/iter. ETA=0:16:57
[08/24 15:54:07] d2.evaluation.evaluator INFO: Inference done 774/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0072 s/iter. Total: 0.8253 s/iter. ETA=0:16:51
[08/24 15:54:13] d2.evaluation.evaluator INFO: Inference done 781/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0072 s/iter. Total: 0.8253 s/iter. ETA=0:16:46
[08/24 15:54:19] d2.evaluation.evaluator INFO: Inference done 788/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0072 s/iter. Total: 0.8253 s/iter. ETA=0:16:40
[08/24 15:54:25] d2.evaluation.evaluator INFO: Inference done 795/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0072 s/iter. Total: 0.8253 s/iter. ETA=0:16:34
[08/24 15:54:31] d2.evaluation.evaluator INFO: Inference done 802/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0073 s/iter. Total: 0.8254 s/iter. ETA=0:16:28
[08/24 15:54:36] d2.evaluation.evaluator INFO: Inference done 809/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0073 s/iter. Total: 0.8254 s/iter. ETA=0:16:23
[08/24 15:54:42] d2.evaluation.evaluator INFO: Inference done 816/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0073 s/iter. Total: 0.8254 s/iter. ETA=0:16:17
[08/24 15:54:48] d2.evaluation.evaluator INFO: Inference done 823/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:16:11
[08/24 15:54:54] d2.evaluation.evaluator INFO: Inference done 830/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0074 s/iter. Total: 0.8255 s/iter. ETA=0:16:05
[08/24 15:55:00] d2.evaluation.evaluator INFO: Inference done 837/2000. Dataloading: 0.0015 s/iter. Inference: 0.8166 s/iter. Eval: 0.0074 s/iter. Total: 0.8255 s/iter. ETA=0:16:00
[08/24 15:55:05] d2.evaluation.evaluator INFO: Inference done 844/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0074 s/iter. Total: 0.8256 s/iter. ETA=0:15:54
[08/24 15:55:11] d2.evaluation.evaluator INFO: Inference done 851/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0074 s/iter. Total: 0.8256 s/iter. ETA=0:15:48
[08/24 15:55:17] d2.evaluation.evaluator INFO: Inference done 858/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0074 s/iter. Total: 0.8256 s/iter. ETA=0:15:42
[08/24 15:55:23] d2.evaluation.evaluator INFO: Inference done 865/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0074 s/iter. Total: 0.8256 s/iter. ETA=0:15:37
[08/24 15:55:28] d2.evaluation.evaluator INFO: Inference done 872/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0074 s/iter. Total: 0.8256 s/iter. ETA=0:15:31
[08/24 15:55:34] d2.evaluation.evaluator INFO: Inference done 879/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:15:25
[08/24 15:55:40] d2.evaluation.evaluator INFO: Inference done 886/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:15:19
[08/24 15:55:46] d2.evaluation.evaluator INFO: Inference done 893/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:15:13
[08/24 15:55:52] d2.evaluation.evaluator INFO: Inference done 900/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:15:08
[08/24 15:55:57] d2.evaluation.evaluator INFO: Inference done 907/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:15:02
[08/24 15:56:03] d2.evaluation.evaluator INFO: Inference done 914/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:56
[08/24 15:56:09] d2.evaluation.evaluator INFO: Inference done 921/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:50
[08/24 15:56:15] d2.evaluation.evaluator INFO: Inference done 928/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:44
[08/24 15:56:20] d2.evaluation.evaluator INFO: Inference done 935/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:39
[08/24 15:56:26] d2.evaluation.evaluator INFO: Inference done 942/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:33
[08/24 15:56:32] d2.evaluation.evaluator INFO: Inference done 949/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:27
[08/24 15:56:38] d2.evaluation.evaluator INFO: Inference done 956/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:21
[08/24 15:56:44] d2.evaluation.evaluator INFO: Inference done 963/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:16
[08/24 15:56:49] d2.evaluation.evaluator INFO: Inference done 970/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:10
[08/24 15:56:55] d2.evaluation.evaluator INFO: Inference done 977/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:14:04
[08/24 15:57:01] d2.evaluation.evaluator INFO: Inference done 984/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:13:58
[08/24 15:57:07] d2.evaluation.evaluator INFO: Inference done 991/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0072 s/iter. Total: 0.8255 s/iter. ETA=0:13:52
[08/24 15:57:12] d2.evaluation.evaluator INFO: Inference done 998/2000. Dataloading: 0.0015 s/iter. Inference: 0.8167 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:13:47
[08/24 15:57:18] d2.evaluation.evaluator INFO: Inference done 1005/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0072 s/iter. Total: 0.8255 s/iter. ETA=0:13:41
[08/24 15:57:24] d2.evaluation.evaluator INFO: Inference done 1012/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:13:35
[08/24 15:57:30] d2.evaluation.evaluator INFO: Inference done 1019/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:13:29
[08/24 15:57:36] d2.evaluation.evaluator INFO: Inference done 1026/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:13:24
[08/24 15:57:41] d2.evaluation.evaluator INFO: Inference done 1033/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:13:18
[08/24 15:57:47] d2.evaluation.evaluator INFO: Inference done 1040/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:13:12
[08/24 15:57:53] d2.evaluation.evaluator INFO: Inference done 1047/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:13:06
[08/24 15:57:59] d2.evaluation.evaluator INFO: Inference done 1054/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:13:00
[08/24 15:58:04] d2.evaluation.evaluator INFO: Inference done 1061/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8255 s/iter. ETA=0:12:55
[08/24 15:58:10] d2.evaluation.evaluator INFO: Inference done 1068/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0072 s/iter. Total: 0.8255 s/iter. ETA=0:12:49
[08/24 15:58:16] d2.evaluation.evaluator INFO: Inference done 1075/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0072 s/iter. Total: 0.8255 s/iter. ETA=0:12:43
[08/24 15:58:22] d2.evaluation.evaluator INFO: Inference done 1082/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:12:37
[08/24 15:58:28] d2.evaluation.evaluator INFO: Inference done 1089/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:12:32
[08/24 15:58:33] d2.evaluation.evaluator INFO: Inference done 1096/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:12:26
[08/24 15:58:39] d2.evaluation.evaluator INFO: Inference done 1103/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:12:20
[08/24 15:58:45] d2.evaluation.evaluator INFO: Inference done 1110/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:12:14
[08/24 15:58:51] d2.evaluation.evaluator INFO: Inference done 1117/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:12:08
[08/24 15:58:57] d2.evaluation.evaluator INFO: Inference done 1124/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:12:03
[08/24 15:59:02] d2.evaluation.evaluator INFO: Inference done 1131/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:11:57
[08/24 15:59:08] d2.evaluation.evaluator INFO: Inference done 1138/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:11:51
[08/24 15:59:14] d2.evaluation.evaluator INFO: Inference done 1145/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:11:45
[08/24 15:59:20] d2.evaluation.evaluator INFO: Inference done 1152/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:11:40
[08/24 15:59:25] d2.evaluation.evaluator INFO: Inference done 1158/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8258 s/iter. ETA=0:11:35
[08/24 15:59:31] d2.evaluation.evaluator INFO: Inference done 1165/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8258 s/iter. ETA=0:11:29
[08/24 15:59:36] d2.evaluation.evaluator INFO: Inference done 1172/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8258 s/iter. ETA=0:11:23
[08/24 15:59:42] d2.evaluation.evaluator INFO: Inference done 1179/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8258 s/iter. ETA=0:11:17
[08/24 15:59:48] d2.evaluation.evaluator INFO: Inference done 1186/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0075 s/iter. Total: 0.8258 s/iter. ETA=0:11:12
[08/24 15:59:54] d2.evaluation.evaluator INFO: Inference done 1193/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8258 s/iter. ETA=0:11:06
[08/24 16:00:00] d2.evaluation.evaluator INFO: Inference done 1200/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8258 s/iter. ETA=0:11:00
[08/24 16:00:05] d2.evaluation.evaluator INFO: Inference done 1207/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8258 s/iter. ETA=0:10:54
[08/24 16:00:11] d2.evaluation.evaluator INFO: Inference done 1214/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8257 s/iter. ETA=0:10:49
[08/24 16:00:17] d2.evaluation.evaluator INFO: Inference done 1221/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8257 s/iter. ETA=0:10:43
[08/24 16:00:23] d2.evaluation.evaluator INFO: Inference done 1228/2000. Dataloading: 0.0015 s/iter. Inference: 0.8168 s/iter. Eval: 0.0074 s/iter. Total: 0.8257 s/iter. ETA=0:10:37
[08/24 16:00:28] d2.evaluation.evaluator INFO: Inference done 1235/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0074 s/iter. Total: 0.8257 s/iter. ETA=0:10:31
[08/24 16:00:34] d2.evaluation.evaluator INFO: Inference done 1242/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:10:25
[08/24 16:00:40] d2.evaluation.evaluator INFO: Inference done 1249/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:10:20
[08/24 16:00:46] d2.evaluation.evaluator INFO: Inference done 1256/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:10:14
[08/24 16:00:51] d2.evaluation.evaluator INFO: Inference done 1263/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:10:08
[08/24 16:00:57] d2.evaluation.evaluator INFO: Inference done 1270/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:10:02
[08/24 16:01:03] d2.evaluation.evaluator INFO: Inference done 1277/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:56
[08/24 16:01:09] d2.evaluation.evaluator INFO: Inference done 1284/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:51
[08/24 16:01:14] d2.evaluation.evaluator INFO: Inference done 1291/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:45
[08/24 16:01:20] d2.evaluation.evaluator INFO: Inference done 1298/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:39
[08/24 16:01:26] d2.evaluation.evaluator INFO: Inference done 1305/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8256 s/iter. ETA=0:09:33
[08/24 16:01:32] d2.evaluation.evaluator INFO: Inference done 1312/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:28
[08/24 16:01:38] d2.evaluation.evaluator INFO: Inference done 1319/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:22
[08/24 16:01:43] d2.evaluation.evaluator INFO: Inference done 1326/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:16
[08/24 16:01:49] d2.evaluation.evaluator INFO: Inference done 1333/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:10
[08/24 16:01:55] d2.evaluation.evaluator INFO: Inference done 1340/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:09:04
[08/24 16:02:01] d2.evaluation.evaluator INFO: Inference done 1347/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:08:59
[08/24 16:02:06] d2.evaluation.evaluator INFO: Inference done 1354/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:08:53
[08/24 16:02:12] d2.evaluation.evaluator INFO: Inference done 1361/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:47
[08/24 16:02:18] d2.evaluation.evaluator INFO: Inference done 1368/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:41
[08/24 16:02:24] d2.evaluation.evaluator INFO: Inference done 1375/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:36
[08/24 16:02:30] d2.evaluation.evaluator INFO: Inference done 1382/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:30
[08/24 16:02:35] d2.evaluation.evaluator INFO: Inference done 1389/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:24
[08/24 16:02:41] d2.evaluation.evaluator INFO: Inference done 1396/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:18
[08/24 16:02:47] d2.evaluation.evaluator INFO: Inference done 1403/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:12
[08/24 16:02:53] d2.evaluation.evaluator INFO: Inference done 1410/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:07
[08/24 16:02:58] d2.evaluation.evaluator INFO: Inference done 1417/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:08:01
[08/24 16:03:04] d2.evaluation.evaluator INFO: Inference done 1424/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:55
[08/24 16:03:10] d2.evaluation.evaluator INFO: Inference done 1431/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:49
[08/24 16:03:16] d2.evaluation.evaluator INFO: Inference done 1438/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:43
[08/24 16:03:22] d2.evaluation.evaluator INFO: Inference done 1445/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:38
[08/24 16:03:27] d2.evaluation.evaluator INFO: Inference done 1452/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:32
[08/24 16:03:33] d2.evaluation.evaluator INFO: Inference done 1459/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:26
[08/24 16:03:39] d2.evaluation.evaluator INFO: Inference done 1466/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:20
[08/24 16:03:45] d2.evaluation.evaluator INFO: Inference done 1473/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:15
[08/24 16:03:50] d2.evaluation.evaluator INFO: Inference done 1480/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:09
[08/24 16:03:56] d2.evaluation.evaluator INFO: Inference done 1487/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:07:03
[08/24 16:04:02] d2.evaluation.evaluator INFO: Inference done 1494/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:06:57
[08/24 16:04:08] d2.evaluation.evaluator INFO: Inference done 1501/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:06:51
[08/24 16:04:14] d2.evaluation.evaluator INFO: Inference done 1508/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:06:46
[08/24 16:04:19] d2.evaluation.evaluator INFO: Inference done 1514/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:06:41
[08/24 16:04:24] d2.evaluation.evaluator INFO: Inference done 1521/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:06:35
[08/24 16:04:30] d2.evaluation.evaluator INFO: Inference done 1528/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:06:29
[08/24 16:04:36] d2.evaluation.evaluator INFO: Inference done 1535/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:06:23
[08/24 16:04:42] d2.evaluation.evaluator INFO: Inference done 1542/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:06:18
[08/24 16:04:48] d2.evaluation.evaluator INFO: Inference done 1549/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:06:12
[08/24 16:04:53] d2.evaluation.evaluator INFO: Inference done 1556/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:06:06
[08/24 16:04:59] d2.evaluation.evaluator INFO: Inference done 1563/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:06:00
[08/24 16:05:05] d2.evaluation.evaluator INFO: Inference done 1570/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:55
[08/24 16:05:11] d2.evaluation.evaluator INFO: Inference done 1577/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:49
[08/24 16:05:16] d2.evaluation.evaluator INFO: Inference done 1584/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:43
[08/24 16:05:22] d2.evaluation.evaluator INFO: Inference done 1591/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:37
[08/24 16:05:28] d2.evaluation.evaluator INFO: Inference done 1598/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0073 s/iter. Total: 0.8257 s/iter. ETA=0:05:31
[08/24 16:05:34] d2.evaluation.evaluator INFO: Inference done 1605/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:26
[08/24 16:05:40] d2.evaluation.evaluator INFO: Inference done 1612/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:20
[08/24 16:05:45] d2.evaluation.evaluator INFO: Inference done 1619/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:14
[08/24 16:05:51] d2.evaluation.evaluator INFO: Inference done 1626/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:08
[08/24 16:05:57] d2.evaluation.evaluator INFO: Inference done 1633/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:05:03
[08/24 16:06:03] d2.evaluation.evaluator INFO: Inference done 1640/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:04:57
[08/24 16:06:08] d2.evaluation.evaluator INFO: Inference done 1647/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:04:51
[08/24 16:06:14] d2.evaluation.evaluator INFO: Inference done 1654/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:04:45
[08/24 16:06:20] d2.evaluation.evaluator INFO: Inference done 1661/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:04:39
[08/24 16:06:26] d2.evaluation.evaluator INFO: Inference done 1668/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:04:34
[08/24 16:06:31] d2.evaluation.evaluator INFO: Inference done 1675/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:04:28
[08/24 16:06:37] d2.evaluation.evaluator INFO: Inference done 1682/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:04:22
[08/24 16:06:43] d2.evaluation.evaluator INFO: Inference done 1689/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:04:16
[08/24 16:06:49] d2.evaluation.evaluator INFO: Inference done 1696/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:04:10
[08/24 16:06:55] d2.evaluation.evaluator INFO: Inference done 1703/2000. Dataloading: 0.0015 s/iter. Inference: 0.8169 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:04:05
[08/24 16:07:00] d2.evaluation.evaluator INFO: Inference done 1710/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:03:59
[08/24 16:07:06] d2.evaluation.evaluator INFO: Inference done 1717/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:03:53
[08/24 16:07:12] d2.evaluation.evaluator INFO: Inference done 1724/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:03:47
[08/24 16:07:18] d2.evaluation.evaluator INFO: Inference done 1731/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:03:42
[08/24 16:07:24] d2.evaluation.evaluator INFO: Inference done 1738/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:03:36
[08/24 16:07:29] d2.evaluation.evaluator INFO: Inference done 1745/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8256 s/iter. ETA=0:03:30
[08/24 16:07:35] d2.evaluation.evaluator INFO: Inference done 1752/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:03:24
[08/24 16:07:41] d2.evaluation.evaluator INFO: Inference done 1759/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:03:18
[08/24 16:07:47] d2.evaluation.evaluator INFO: Inference done 1766/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:03:13
[08/24 16:07:52] d2.evaluation.evaluator INFO: Inference done 1773/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:03:07
[08/24 16:07:58] d2.evaluation.evaluator INFO: Inference done 1780/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:03:01
[08/24 16:08:04] d2.evaluation.evaluator INFO: Inference done 1787/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:02:55
[08/24 16:08:10] d2.evaluation.evaluator INFO: Inference done 1794/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0071 s/iter. Total: 0.8256 s/iter. ETA=0:02:50
[08/24 16:08:16] d2.evaluation.evaluator INFO: Inference done 1801/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:02:44
[08/24 16:08:21] d2.evaluation.evaluator INFO: Inference done 1808/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:02:38
[08/24 16:08:27] d2.evaluation.evaluator INFO: Inference done 1815/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:02:32
[08/24 16:08:33] d2.evaluation.evaluator INFO: Inference done 1822/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:02:26
[08/24 16:08:39] d2.evaluation.evaluator INFO: Inference done 1829/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:02:21
[08/24 16:08:45] d2.evaluation.evaluator INFO: Inference done 1836/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:02:15
[08/24 16:08:50] d2.evaluation.evaluator INFO: Inference done 1843/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:02:09
[08/24 16:08:56] d2.evaluation.evaluator INFO: Inference done 1850/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8258 s/iter. ETA=0:02:03
[08/24 16:09:02] d2.evaluation.evaluator INFO: Inference done 1857/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8258 s/iter. ETA=0:01:58
[08/24 16:09:08] d2.evaluation.evaluator INFO: Inference done 1864/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8258 s/iter. ETA=0:01:52
[08/24 16:09:13] d2.evaluation.evaluator INFO: Inference done 1871/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:46
[08/24 16:09:19] d2.evaluation.evaluator INFO: Inference done 1878/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:40
[08/24 16:09:25] d2.evaluation.evaluator INFO: Inference done 1885/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:34
[08/24 16:09:31] d2.evaluation.evaluator INFO: Inference done 1892/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:29
[08/24 16:09:37] d2.evaluation.evaluator INFO: Inference done 1899/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:23
[08/24 16:09:42] d2.evaluation.evaluator INFO: Inference done 1906/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:17
[08/24 16:09:48] d2.evaluation.evaluator INFO: Inference done 1913/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:11
[08/24 16:09:54] d2.evaluation.evaluator INFO: Inference done 1920/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:06
[08/24 16:10:00] d2.evaluation.evaluator INFO: Inference done 1927/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:01:00
[08/24 16:10:05] d2.evaluation.evaluator INFO: Inference done 1934/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:54
[08/24 16:10:11] d2.evaluation.evaluator INFO: Inference done 1941/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:48
[08/24 16:10:17] d2.evaluation.evaluator INFO: Inference done 1948/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:42
[08/24 16:10:23] d2.evaluation.evaluator INFO: Inference done 1955/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:37
[08/24 16:10:29] d2.evaluation.evaluator INFO: Inference done 1962/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:31
[08/24 16:10:34] d2.evaluation.evaluator INFO: Inference done 1969/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:25
[08/24 16:10:40] d2.evaluation.evaluator INFO: Inference done 1976/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:19
[08/24 16:10:46] d2.evaluation.evaluator INFO: Inference done 1983/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:14
[08/24 16:10:52] d2.evaluation.evaluator INFO: Inference done 1990/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:08
[08/24 16:10:57] d2.evaluation.evaluator INFO: Inference done 1997/2000. Dataloading: 0.0015 s/iter. Inference: 0.8170 s/iter. Eval: 0.0072 s/iter. Total: 0.8257 s/iter. ETA=0:00:02
[08/24 16:11:00] d2.evaluation.evaluator INFO: Total inference time: 0:27:27.343839 (0.825736 s / iter per device, on 1 devices)
[08/24 16:11:00] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:27:09 (0.817006 s / iter per device, on 1 devices)
[08/24 16:11:00] d2.evaluation.sem_seg_evaluation INFO: OrderedDict({'sem_seg': {'mIoU': 37.79342244034116, 'fwIoU': 64.72120423599192, 'IoU-wall': 73.88573431296095, 'BoundaryIoU-wall': 73.86020610163398, 'min(IoU, B-Iou)-wall': 73.86020610163398, 'IoU-building': 65.89399131903197, 'BoundaryIoU-building': 8.670766838341146, 'min(IoU, B-Iou)-building': 8.670766838341146, 'IoU-sky': 92.89689539733995, 'BoundaryIoU-sky': 0.0, 'min(IoU, B-Iou)-sky': 0.0, 'IoU-floor': 78.55686841523557, 'BoundaryIoU-floor': 0.0, 'min(IoU, B-Iou)-floor': 0.0, 'IoU-tree': 71.60351912375562, 'BoundaryIoU-tree': 0.0, 'min(IoU, B-Iou)-tree': 0.0, 'IoU-ceiling': 81.2525033336575, 'BoundaryIoU-ceiling': 0.0, 'min(IoU, B-Iou)-ceiling': 0.0, 'IoU-road': 78.38213438220535, 'BoundaryIoU-road': 0.0, 'min(IoU, B-Iou)-road': 0.0, 'IoU-bed ': 81.36872274636, 'BoundaryIoU-bed ': 0.0, 'min(IoU, B-Iou)-bed ': 0.0, 'IoU-windowpane': 50.12541745598191, 'BoundaryIoU-windowpane': 0.0, 'min(IoU, B-Iou)-windowpane': 0.0, 'IoU-grass': 62.35558707830558, 'BoundaryIoU-grass': 0.0, 'min(IoU, B-Iou)-grass': 0.0, 'IoU-cabinet': 58.58584167601569, 'BoundaryIoU-cabinet': 0.0, 'min(IoU, B-Iou)-cabinet': 0.0, 'IoU-sidewalk': 62.188678052633406, 'BoundaryIoU-sidewalk': 0.0, 'min(IoU, B-Iou)-sidewalk': 0.0, 'IoU-person': 82.25816334566588, 'BoundaryIoU-person': 0.0, 'min(IoU, B-Iou)-person': 0.0, 'IoU-earth': 4.730099122735162, 'BoundaryIoU-earth': 0.0, 'min(IoU, B-Iou)-earth': 0.0, 'IoU-door': 44.59167544241557, 'BoundaryIoU-door': 0.0, 'min(IoU, B-Iou)-door': 0.0, 'IoU-table': 44.155954679089746, 'BoundaryIoU-table': 0.0, 'min(IoU, B-Iou)-table': 0.0, 'IoU-mountain': 51.64682221146323, 'BoundaryIoU-mountain': 0.0, 'min(IoU, B-Iou)-mountain': 0.0, 'IoU-plant': 45.01738794034912, 'BoundaryIoU-plant': 0.0, 'min(IoU, B-Iou)-plant': 0.0, 'IoU-curtain': 71.82813693466854, 'BoundaryIoU-curtain': 0.0, 'min(IoU, B-Iou)-curtain': 0.0, 'IoU-chair': 51.58591854213256, 'BoundaryIoU-chair': 0.0, 'min(IoU, B-Iou)-chair': 0.0, 'IoU-car': 73.45956795205252, 'BoundaryIoU-car': 0.0, 'min(IoU, B-Iou)-car': 0.0, 'IoU-water': 25.530251594631277, 'BoundaryIoU-water': 0.0, 'min(IoU, B-Iou)-water': 0.0, 'IoU-painting': 62.105369948655174, 'BoundaryIoU-painting': 0.0, 'min(IoU, B-Iou)-painting': 0.0, 'IoU-sofa': 50.51727331450814, 'BoundaryIoU-sofa': 0.0, 'min(IoU, B-Iou)-sofa': 0.0, 'IoU-shelf': 35.4520778861796, 'BoundaryIoU-shelf': 0.0, 'min(IoU, B-Iou)-shelf': 0.0, 'IoU-house': 24.127403078296858, 'BoundaryIoU-house': 0.0, 'min(IoU, B-Iou)-house': 0.0, 'IoU-sea': 48.38636466296709, 'BoundaryIoU-sea': 0.0, 'min(IoU, B-Iou)-sea': 0.0, 'IoU-mirror': 63.883159764135975, 'BoundaryIoU-mirror': 0.0, 'min(IoU, B-Iou)-mirror': 0.0, 'IoU-rug': 53.535244510310484, 'BoundaryIoU-rug': 0.0, 'min(IoU, B-Iou)-rug': 0.0, 'IoU-field': 19.593244206113678, 'BoundaryIoU-field': 0.0, 'min(IoU, B-Iou)-field': 0.0, 'IoU-armchair': 36.49122884381805, 'BoundaryIoU-armchair': 0.0, 'min(IoU, B-Iou)-armchair': 0.0, 'IoU-seat': 55.011218394554064, 'BoundaryIoU-seat': 0.0, 'min(IoU, B-Iou)-seat': 0.0, 'IoU-fence': 32.59863303431214, 'BoundaryIoU-fence': 0.0, 'min(IoU, B-Iou)-fence': 0.0, 'IoU-desk': 35.51932418255468, 'BoundaryIoU-desk': 0.0, 'min(IoU, B-Iou)-desk': 0.0, 'IoU-rock': 33.425903084494394, 'BoundaryIoU-rock': 0.0, 'min(IoU, B-Iou)-rock': 0.0, 'IoU-wardrobe': 32.634319172154534, 'BoundaryIoU-wardrobe': 0.0, 'min(IoU, B-Iou)-wardrobe': 0.0, 'IoU-lamp': 52.67003717854708, 'BoundaryIoU-lamp': 0.0, 'min(IoU, B-Iou)-lamp': 0.0, 'IoU-bathtub': 70.99120549468995, 'BoundaryIoU-bathtub': 0.0, 'min(IoU, B-Iou)-bathtub': 0.0, 'IoU-railing': 18.889843258676056, 'BoundaryIoU-railing': 0.0, 'min(IoU, B-Iou)-railing': 0.0, 'IoU-cushion': 12.932288569915817, 'BoundaryIoU-cushion': 0.0, 'min(IoU, B-Iou)-cushion': 0.0, 'IoU-base': 0.0021652754771725835, 'BoundaryIoU-base': 0.0, 'min(IoU, B-Iou)-base': 0.0, 'IoU-box': 22.37456576161382, 'BoundaryIoU-box': 0.0, 'min(IoU, B-Iou)-box': 0.0, 'IoU-column': 39.24797952979336, 'BoundaryIoU-column': 0.0, 'min(IoU, B-Iou)-column': 0.0, 'IoU-signboard': 24.58717946223904, 'BoundaryIoU-signboard': 0.0, 'min(IoU, B-Iou)-signboard': 0.0, 'IoU-chest of drawers': 29.62527020614752, 'BoundaryIoU-chest of drawers': 0.0, 'min(IoU, B-Iou)-chest of drawers': 0.0, 'IoU-counter': 22.751522014231863, 'BoundaryIoU-counter': 0.0, 'min(IoU, B-Iou)-counter': 0.0, 'IoU-sand': 38.18911792314837, 'BoundaryIoU-sand': 0.0, 'min(IoU, B-Iou)-sand': 0.0, 'IoU-sink': 64.85881538868877, 'BoundaryIoU-sink': 0.0, 'min(IoU, B-Iou)-sink': 0.0, 'IoU-skyscraper': 28.854210981832345, 'BoundaryIoU-skyscraper': 0.0, 'min(IoU, B-Iou)-skyscraper': 0.0, 'IoU-fireplace': 63.05626857332567, 'BoundaryIoU-fireplace': 0.0, 'min(IoU, B-Iou)-fireplace': 0.0, 'IoU-refrigerator': 61.60946184826782, 'BoundaryIoU-refrigerator': 0.0, 'min(IoU, B-Iou)-refrigerator': 0.0, 'IoU-grandstand': 54.976873063297695, 'BoundaryIoU-grandstand': 0.0, 'min(IoU, B-Iou)-grandstand': 0.0, 'IoU-path': 16.641143308034927, 'BoundaryIoU-path': 0.0, 'min(IoU, B-Iou)-path': 0.0, 'IoU-stairs': 17.141752892956756, 'BoundaryIoU-stairs': 0.0, 'min(IoU, B-Iou)-stairs': 0.0, 'IoU-runway': 24.98767472990469, 'BoundaryIoU-runway': 0.0, 'min(IoU, B-Iou)-runway': 0.0, 'IoU-case': 0.0, 'BoundaryIoU-case': 0.0, 'min(IoU, B-Iou)-case': 0.0, 'IoU-pool table': 79.4801752734445, 'BoundaryIoU-pool table': 0.0, 'min(IoU, B-Iou)-pool table': 0.0, 'IoU-pillow': 0.6212466349140608, 'BoundaryIoU-pillow': 0.0, 'min(IoU, B-Iou)-pillow': 0.0, 'IoU-screen door': 0.46664896992667576, 'BoundaryIoU-screen door': 0.0, 'min(IoU, B-Iou)-screen door': 0.0, 'IoU-stairway': 6.5207310080031915, 'BoundaryIoU-stairway': 0.0, 'min(IoU, B-Iou)-stairway': 0.0, 'IoU-river': 16.01412071774056, 'BoundaryIoU-river': 0.0, 'min(IoU, B-Iou)-river': 0.0, 'IoU-bridge': 54.02961382059909, 'BoundaryIoU-bridge': 0.0, 'min(IoU, B-Iou)-bridge': 0.0, 'IoU-bookcase': 16.562811651669588, 'BoundaryIoU-bookcase': 0.0, 'min(IoU, B-Iou)-bookcase': 0.0, 'IoU-blind': 47.18701124794473, 'BoundaryIoU-blind': 0.0, 'min(IoU, B-Iou)-blind': 0.0, 'IoU-coffee table': 53.06245542569664, 'BoundaryIoU-coffee table': 0.0, 'min(IoU, B-Iou)-coffee table': 0.0, 'IoU-toilet': 80.26519869016514, 'BoundaryIoU-toilet': 0.0, 'min(IoU, B-Iou)-toilet': 0.0, 'IoU-flower': 31.25654312455052, 'BoundaryIoU-flower': 0.0, 'min(IoU, B-Iou)-flower': 0.0, 'IoU-book': 37.79027707674058, 'BoundaryIoU-book': 0.0, 'min(IoU, B-Iou)-book': 0.0, 'IoU-hill': 4.234180779596927, 'BoundaryIoU-hill': 0.0, 'min(IoU, B-Iou)-hill': 0.0, 'IoU-bench': 47.1125541658322, 'BoundaryIoU-bench': 0.0, 'min(IoU, B-Iou)-bench': 0.0, 'IoU-countertop': 41.25193201377492, 'BoundaryIoU-countertop': 0.0, 'min(IoU, B-Iou)-countertop': 0.0, 'IoU-stove': 21.28887244830501, 'BoundaryIoU-stove': 0.0, 'min(IoU, B-Iou)-stove': 0.0, 'IoU-palm': 36.816846742893105, 'BoundaryIoU-palm': 0.0, 'min(IoU, B-Iou)-palm': 0.0, 'IoU-kitchen island': 32.42563187071315, 'BoundaryIoU-kitchen island': 0.0, 'min(IoU, B-Iou)-kitchen island': 0.0, 'IoU-computer': 29.464279516801135, 'BoundaryIoU-computer': 0.0, 'min(IoU, B-Iou)-computer': 0.0, 'IoU-swivel chair': 7.0655545257954895, 'BoundaryIoU-swivel chair': 0.0, 'min(IoU, B-Iou)-swivel chair': 0.0, 'IoU-boat': 80.72058922133452, 'BoundaryIoU-boat': 0.0, 'min(IoU, B-Iou)-boat': 0.0, 'IoU-bar': 37.8211068785458, 'BoundaryIoU-bar': 0.0, 'min(IoU, B-Iou)-bar': 0.0, 'IoU-arcade machine': 82.55993992598023, 'BoundaryIoU-arcade machine': 0.0, 'min(IoU, B-Iou)-arcade machine': 0.0, 'IoU-hovel': 0.6871698480085556, 'BoundaryIoU-hovel': 0.0, 'min(IoU, B-Iou)-hovel': 0.0, 'IoU-bus': 70.37749595669226, 'BoundaryIoU-bus': 0.0, 'min(IoU, B-Iou)-bus': 0.0, 'IoU-towel': 52.950648287256875, 'BoundaryIoU-towel': 0.0, 'min(IoU, B-Iou)-towel': 0.0, 'IoU-light': 27.91664518999395, 'BoundaryIoU-light': 0.0, 'min(IoU, B-Iou)-light': 0.0, 'IoU-truck': 16.171961870216233, 'BoundaryIoU-truck': 0.0, 'min(IoU, B-Iou)-truck': 0.0, 'IoU-tower': 19.18360695400753, 'BoundaryIoU-tower': 0.0, 'min(IoU, B-Iou)-tower': 0.0, 'IoU-chandelier': 61.367403586826, 'BoundaryIoU-chandelier': 0.0, 'min(IoU, B-Iou)-chandelier': 0.0, 'IoU-awning': 27.65144688094523, 'BoundaryIoU-awning': 0.0, 'min(IoU, B-Iou)-awning': 0.0, 'IoU-streetlight': 17.866922439576864, 'BoundaryIoU-streetlight': 0.0, 'min(IoU, B-Iou)-streetlight': 0.0, 'IoU-booth': 2.045909536219201, 'BoundaryIoU-booth': 0.0, 'min(IoU, B-Iou)-booth': 0.0, 'IoU-television receiver': 65.82937518643446, 'BoundaryIoU-television receiver': 0.0, 'min(IoU, B-Iou)-television receiver': 0.0, 'IoU-airplane': 23.23698458017418, 'BoundaryIoU-airplane': 0.0, 'min(IoU, B-Iou)-airplane': 0.0, 'IoU-dirt track': 1.5206396350950122, 'BoundaryIoU-dirt track': 0.0, 'min(IoU, B-Iou)-dirt track': 0.0, 'IoU-apparel': 27.421152435931813, 'BoundaryIoU-apparel': 0.0, 'min(IoU, B-Iou)-apparel': 0.0, 'IoU-pole': 18.24778208728991, 'BoundaryIoU-pole': 0.0, 'min(IoU, B-Iou)-pole': 0.0, 'IoU-land': 6.629741987805245, 'BoundaryIoU-land': 0.0, 'min(IoU, B-Iou)-land': 0.0, 'IoU-bannister': 6.978392886766477, 'BoundaryIoU-bannister': 0.0, 'min(IoU, B-Iou)-bannister': 0.0, 'IoU-escalator': 61.84600399732868, 'BoundaryIoU-escalator': 0.0, 'min(IoU, B-Iou)-escalator': 0.0, 'IoU-ottoman': 1.4907487981283614, 'BoundaryIoU-ottoman': 0.0, 'min(IoU, B-Iou)-ottoman': 0.0, 'IoU-bottle': 34.607977920921066, 'BoundaryIoU-bottle': 0.0, 'min(IoU, B-Iou)-bottle': 0.0, 'IoU-buffet': 0.0, 'BoundaryIoU-buffet': 0.0, 'min(IoU, B-Iou)-buffet': 0.0, 'IoU-poster': 21.707365622596186, 'BoundaryIoU-poster': 0.0, 'min(IoU, B-Iou)-poster': 0.0, 'IoU-stage': 12.849389416553594, 'BoundaryIoU-stage': 0.0, 'min(IoU, B-Iou)-stage': 0.0, 'IoU-van': 35.78622008231206, 'BoundaryIoU-van': 0.0, 'min(IoU, B-Iou)-van': 0.0, 'IoU-ship': 76.13541223642574, 'BoundaryIoU-ship': 0.0, 'min(IoU, B-Iou)-ship': 0.0, 'IoU-fountain': 57.66462082334927, 'BoundaryIoU-fountain': 0.0, 'min(IoU, B-Iou)-fountain': 0.0, 'IoU-conveyer belt': 66.83525127012302, 'BoundaryIoU-conveyer belt': 0.0, 'min(IoU, B-Iou)-conveyer belt': 0.0, 'IoU-canopy': 1.6158420492599617, 'BoundaryIoU-canopy': 0.0, 'min(IoU, B-Iou)-canopy': 0.0, 'IoU-washer': 84.39702391912701, 'BoundaryIoU-washer': 0.0, 'min(IoU, B-Iou)-washer': 0.0, 'IoU-plaything': 8.850194823800528, 'BoundaryIoU-plaything': 0.0, 'min(IoU, B-Iou)-plaything': 0.0, 'IoU-swimming pool': 29.015103543704118, 'BoundaryIoU-swimming pool': 0.0, 'min(IoU, B-Iou)-swimming pool': 0.0, 'IoU-stool': 20.93838674752507, 'BoundaryIoU-stool': 0.0, 'min(IoU, B-Iou)-stool': 0.0, 'IoU-barrel': 11.931600895962704, 'BoundaryIoU-barrel': 0.0, 'min(IoU, B-Iou)-barrel': 0.0, 'IoU-basket': 39.15975864207974, 'BoundaryIoU-basket': 0.0, 'min(IoU, B-Iou)-basket': 0.0, 'IoU-waterfall': 43.80351653071695, 'BoundaryIoU-waterfall': 0.0, 'min(IoU, B-Iou)-waterfall': 0.0, 'IoU-tent': 85.57210540239376, 'BoundaryIoU-tent': 0.0, 'min(IoU, B-Iou)-tent': 0.0, 'IoU-bag': 27.03809758251942, 'BoundaryIoU-bag': 0.0, 'min(IoU, B-Iou)-bag': 0.0, 'IoU-minibike': 72.36794771868817, 'BoundaryIoU-minibike': 0.0, 'min(IoU, B-Iou)-minibike': 0.0, 'IoU-cradle': 55.58706650047658, 'BoundaryIoU-cradle': 0.0, 'min(IoU, B-Iou)-cradle': 0.0, 'IoU-oven': 16.33271305255206, 'BoundaryIoU-oven': 0.0, 'min(IoU, B-Iou)-oven': 0.0, 'IoU-ball': 23.54738284970843, 'BoundaryIoU-ball': 0.0, 'min(IoU, B-Iou)-ball': 0.0, 'IoU-food': 52.82846060120902, 'BoundaryIoU-food': 0.0, 'min(IoU, B-Iou)-food': 0.0, 'IoU-step': 3.591251363025847, 'BoundaryIoU-step': 0.0, 'min(IoU, B-Iou)-step': 0.0, 'IoU-tank': 38.46160545165574, 'BoundaryIoU-tank': 0.0, 'min(IoU, B-Iou)-tank': 0.0, 'IoU-trade name': 5.520448107640255, 'BoundaryIoU-trade name': 0.0, 'min(IoU, B-Iou)-trade name': 0.0, 'IoU-microwave': 85.60446937052035, 'BoundaryIoU-microwave': 0.0, 'min(IoU, B-Iou)-microwave': 0.0, 'IoU-pot': 44.8816646232893, 'BoundaryIoU-pot': 0.0, 'min(IoU, B-Iou)-pot': 0.0, 'IoU-animal': 78.27010928289087, 'BoundaryIoU-animal': 0.0, 'min(IoU, B-Iou)-animal': 0.0, 'IoU-bicycle': 51.1779778005308, 'BoundaryIoU-bicycle': 0.0, 'min(IoU, B-Iou)-bicycle': 0.0, 'IoU-lake': 2.497113742464379, 'BoundaryIoU-lake': 0.0, 'min(IoU, B-Iou)-lake': 0.0, 'IoU-dishwasher': 6.520936461547127, 'BoundaryIoU-dishwasher': 0.0, 'min(IoU, B-Iou)-dishwasher': 0.0, 'IoU-screen': 29.201298882292274, 'BoundaryIoU-screen': 0.0, 'min(IoU, B-Iou)-screen': 0.0, 'IoU-blanket': 0.0, 'BoundaryIoU-blanket': 0.0, 'min(IoU, B-Iou)-blanket': 0.0, 'IoU-sculpture': 52.226683468616244, 'BoundaryIoU-sculpture': 0.0, 'min(IoU, B-Iou)-sculpture': 0.0, 'IoU-hood': 7.8865303436803496, 'BoundaryIoU-hood': 0.0, 'min(IoU, B-Iou)-hood': 0.0, 'IoU-sconce': 14.468004473735292, 'BoundaryIoU-sconce': 0.0, 'min(IoU, B-Iou)-sconce': 0.0, 'IoU-vase': 32.62013168080307, 'BoundaryIoU-vase': 0.0, 'min(IoU, B-Iou)-vase': 0.0, 'IoU-traffic light': 22.896591321492604, 'BoundaryIoU-traffic light': 0.0, 'min(IoU, B-Iou)-traffic light': 0.0, 'IoU-tray': 12.571431846710265, 'BoundaryIoU-tray': 0.0, 'min(IoU, B-Iou)-tray': 0.0, 'IoU-ashcan': 43.23584887124978, 'BoundaryIoU-ashcan': 0.0, 'min(IoU, B-Iou)-ashcan': 0.0, 'IoU-fan': 50.08032558637679, 'BoundaryIoU-fan': 0.0, 'min(IoU, B-Iou)-fan': 0.0, 'IoU-pier': 15.290621517527224, 'BoundaryIoU-pier': 0.0, 'min(IoU, B-Iou)-pier': 0.0, 'IoU-crt screen': 0.23224257148524655, 'BoundaryIoU-crt screen': 0.0, 'min(IoU, B-Iou)-crt screen': 0.0, 'IoU-plate': 54.62306617685104, 'BoundaryIoU-plate': 0.0, 'min(IoU, B-Iou)-plate': 0.0, 'IoU-monitor': 0.19436406121848934, 'BoundaryIoU-monitor': 0.0, 'min(IoU, B-Iou)-monitor': 0.0, 'IoU-bulletin board': 18.133426224779157, 'BoundaryIoU-bulletin board': 0.0, 'min(IoU, B-Iou)-bulletin board': 0.0, 'IoU-shower': 0.5762192433195168, 'BoundaryIoU-shower': 0.0, 'min(IoU, B-Iou)-shower': 0.0, 'IoU-radiator': 58.67226040685106, 'BoundaryIoU-radiator': 0.0, 'min(IoU, B-Iou)-radiator': 0.0, 'IoU-glass': 8.533129570399815, 'BoundaryIoU-glass': 0.0, 'min(IoU, B-Iou)-glass': 0.0, 'IoU-clock': 37.00348878148706, 'BoundaryIoU-clock': 0.0, 'min(IoU, B-Iou)-clock': 0.0, 'IoU-flag': 62.46432210853457, 'BoundaryIoU-flag': 0.0, 'min(IoU, B-Iou)-flag': 0.0, 'mACC': 55.579372548391994, 'pACC': 76.0732008088613, 'ACC-wall': 80.76197927381742, 'ACC-building': 70.8398998365506, 'ACC-sky': 95.47137525767423, 'ACC-floor': 82.82296480943873, 'ACC-tree': 94.05887146780888, 'ACC-ceiling': 91.85533991604474, 'ACC-road': 88.685465794598, 'ACC-bed ': 95.36455710534267, 'ACC-windowpane': 54.860748141549095, 'ACC-grass': 85.47157569335367, 'ACC-cabinet': 75.50363908536994, 'ACC-sidewalk': 85.69867539496468, 'ACC-person': 96.05790319055932, 'ACC-earth': 4.740350320516161, 'ACC-door': 68.08483398920711, 'ACC-table': 55.13711443321454, 'ACC-mountain': 58.96837245469118, 'ACC-plant': 58.43544547950623, 'ACC-curtain': 91.96423945990207, 'ACC-chair': 81.87280391155377, 'ACC-car': 82.01151848372066, 'ACC-water': 29.79732933462284, 'ACC-painting': 79.00189499365112, 'ACC-sofa': 62.87806758494432, 'ACC-shelf': 64.15118062028004, 'ACC-house': 88.78922196005728, 'ACC-sea': 70.42728345179304, 'ACC-mirror': 78.42723033598176, 'ACC-rug': 88.91770129204932, 'ACC-field': 29.675257027112906, 'ACC-armchair': 75.81171666228636, 'ACC-seat': 69.92161617115859, 'ACC-fence': 65.97195277766107, 'ACC-desk': 50.43542587459735, 'ACC-rock': 91.01417555258523, 'ACC-wardrobe': 41.61787254406492, 'ACC-lamp': 65.31704900693103, 'ACC-bathtub': 80.98600684611766, 'ACC-railing': 23.847282214542165, 'ACC-cushion': 13.224753702673459, 'ACC-base': 0.003137258417096266, 'ACC-box': 27.08744403865629, 'ACC-column': 76.33162763902538, 'ACC-signboard': 27.644174686497518, 'ACC-chest of drawers': 59.744208949629915, 'ACC-counter': 31.797885643903623, 'ACC-sand': 91.45510536489977, 'ACC-sink': 74.8726997666537, 'ACC-skyscraper': 95.27930129680676, 'ACC-fireplace': 90.6795440439607, 'ACC-refrigerator': 88.98038939066772, 'ACC-grandstand': 67.76274614641748, 'ACC-path': 23.44416775453339, 'ACC-stairs': 33.65247909541483, 'ACC-runway': 27.46686735605401, 'ACC-case': 0.0, 'ACC-pool table': 80.4022849239516, 'ACC-pillow': 0.6443089707714211, 'ACC-screen door': 0.7293814751668207, 'ACC-stairway': 6.824491698790654, 'ACC-river': 65.59289335546681, 'ACC-bridge': 91.7735985069687, 'ACC-bookcase': 53.62773515666912, 'ACC-blind': 84.7598153094006, 'ACC-coffee table': 90.38669906259747, 'ACC-toilet': 95.14774502662759, 'ACC-flower': 73.13079702731319, 'ACC-book': 53.286088711846055, 'ACC-hill': 8.534985880774135, 'ACC-bench': 63.50425288049707, 'ACC-countertop': 69.29118146543965, 'ACC-stove': 21.78156420759836, 'ACC-palm': 87.18368955946042, 'ACC-kitchen island': 50.77746584976841, 'ACC-computer': 33.36075290295961, 'ACC-swivel chair': 7.34998084670422, 'ACC-boat': 91.19240876592917, 'ACC-bar': 46.46746142333695, 'ACC-arcade machine': 89.66643494335725, 'ACC-hovel': 1.312466575554103, 'ACC-bus': 96.50474843116369, 'ACC-towel': 78.14419308462287, 'ACC-light': 47.70271992953911, 'ACC-truck': 67.65001411233418, 'ACC-tower': 54.33739136344714, 'ACC-chandelier': 82.09136419536885, 'ACC-awning': 40.57627106192122, 'ACC-streetlight': 21.26429459608211, 'ACC-booth': 3.7158158654471674, 'ACC-television receiver': 78.6860741301474, 'ACC-airplane': 69.05426131387486, 'ACC-dirt track': 28.93860982319783, 'ACC-apparel': 51.59025848917847, 'ACC-pole': 28.24926291147208, 'ACC-land': 28.567804595201856, 'ACC-bannister': 20.094148580014046, 'ACC-escalator': 82.50689103955794, 'ACC-ottoman': 1.5194143348382079, 'ACC-bottle': 53.104704264968476, 'ACC-buffet': 0.0, 'ACC-poster': 51.53586761501986, 'ACC-stage': 25.80397451061748, 'ACC-van': 71.15402131341652, 'ACC-ship': 96.2118036776636, 'ACC-fountain': 61.04343058213536, 'ACC-conveyer belt': 75.17459497837733, 'ACC-canopy': 2.675619773834849, 'ACC-washer': 88.57948656959853, 'ACC-plaything': 41.61023542633202, 'ACC-swimming pool': 85.87914757739831, 'ACC-stool': 28.686450537092334, 'ACC-barrel': 74.54773011719195, 'ACC-basket': 51.13342243134483, 'ACC-waterfall': 51.5880636443262, 'ACC-tent': 97.41414712941584, 'ACC-bag': 43.84910733680496, 'ACC-minibike': 79.36124397981492, 'ACC-cradle': 97.2797914164588, 'ACC-oven': 81.59316386034887, 'ACC-ball': 24.292938977038492, 'ACC-food': 86.95215784751973, 'ACC-step': 11.648470082842842, 'ACC-tank': 40.83945377247918, 'ACC-trade name': 6.566616748952899, 'ACC-microwave': 93.60671037910643, 'ACC-pot': 62.7104998728829, 'ACC-animal': 93.3741390317179, 'ACC-bicycle': 87.59961635713664, 'ACC-lake': 6.133381600172744, 'ACC-dishwasher': 6.520936461547127, 'ACC-screen': 77.72382633984213, 'ACC-blanket': 0.0, 'ACC-sculpture': 83.8004878927291, 'ACC-hood': 9.129480928396333, 'ACC-sconce': 17.645467304733078, 'ACC-vase': 59.40368189293862, 'ACC-traffic light': 25.63432383262583, 'ACC-tray': 15.316126869736458, 'ACC-ashcan': 48.55048482642221, 'ACC-fan': 70.95685531842571, 'ACC-pier': 32.64962498086637, 'ACC-crt screen': 1.131339735422136, 'ACC-plate': 76.77706506087183, 'ACC-monitor': 0.2748166430359362, 'ACC-bulletin board': 41.513454241871074, 'ACC-shower': 25.309897346503966, 'ACC-radiator': 87.68139577023634, 'ACC-glass': 27.03182938497286, 'ACC-clock': 43.849800872368675, 'ACC-flag': 81.05050287485267}})
[08/24 16:11:00] d2.engine.defaults INFO: Evaluation results for ade20k_150_test_sem_seg in csv format:
[08/24 16:11:00] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[08/24 16:11:00] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[08/24 16:11:00] d2.evaluation.testing INFO: copypaste: 37.7934,64.7212,55.5794,76.0732
[08/24 16:11:02] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 16:11:03] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 16:11:03] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/ade847.json', 'DATASETS.TEST', '("ade20k_full_sem_seg_freq_val_all",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 16:11:03] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 16:11:03] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ade20k_full_sem_seg_freq_val_all
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/ade847.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 16:11:03] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 16:11:03] d2.utils.env INFO: Using a generated random seed 3629255
[08/24 16:11:11] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 16:11:11] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:11] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:15] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 16:11:15] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 16:11:15] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/voc20.json', 'DATASETS.TEST', '("voc_2012_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 16:11:15] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 16:11:15] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - voc_2012_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/voc20.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 16:11:15] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 16:11:15] d2.utils.env INFO: Using a generated random seed 15686687
[08/24 16:11:20] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 16:11:20] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:20] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:23] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 16:11:23] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 16:11:23] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/voc20b.json', 'DATASETS.TEST', '("voc_2012_test_background_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 16:11:23] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 16:11:23] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - voc_2012_test_background_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/voc20b.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 16:11:23] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 16:11:24] d2.utils.env INFO: Using a generated random seed 24228409
[08/24 16:11:28] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 16:11:28] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:28] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:32] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 16:11:32] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 16:11:32] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/pc59.json', 'DATASETS.TEST', '("context_59_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 16:11:32] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 16:11:32] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - context_59_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/pc59.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 16:11:32] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 16:11:32] d2.utils.env INFO: Using a generated random seed 32900741
[08/24 16:11:37] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 16:11:37] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:37] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:40] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 16:11:41] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 16:11:41] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/pc459.json', 'DATASETS.TEST', '("context_459_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 16:11:41] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 16:11:41] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - context_459_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/pc459.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 16:11:41] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 16:11:41] d2.utils.env INFO: Using a generated random seed 41645137
[08/24 16:11:48] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 16:11:48] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:11:48] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:22:17] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 16:22:17] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 16:22:17] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/ade150.json', 'DATASETS.TEST', '("ade20k_150_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 16:22:17] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 16:22:17] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ade20k_150_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/ade150.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 16:22:17] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 16:22:17] d2.utils.env INFO: Using a generated random seed 17983681
[08/24 16:22:23] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 16:22:23] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:22:23] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:22:24] d2.data.datasets.coco INFO: Loaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation
[08/24 16:22:24] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]
[08/24 16:22:24] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[08/24 16:22:24] d2.data.common INFO: Serializing 2000 elements to byte tensors and concatenating them all ...
[08/24 16:22:24] d2.data.common INFO: Serialized dataset takes 0.39 MiB
[08/24 16:22:24] d2.data.datasets.coco INFO: Loaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation
[08/24 16:22:24] d2.evaluation.evaluator INFO: Start inference on 2000 batches
[08/24 16:22:33] d2.evaluation.evaluator INFO: Inference done 11/2000. Dataloading: 0.0012 s/iter. Inference: 0.8105 s/iter. Eval: 0.0070 s/iter. Total: 0.8188 s/iter. ETA=0:27:08
[08/24 16:22:39] d2.evaluation.evaluator INFO: Inference done 18/2000. Dataloading: 0.0013 s/iter. Inference: 0.8106 s/iter. Eval: 0.0068 s/iter. Total: 0.8187 s/iter. ETA=0:27:02
[08/24 16:24:17] detectron2 INFO: Rank of current process: 0. World size: 1
[08/24 16:24:18] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/detectron2
Compiler                         GCC 11.4
CUDA compiler                    not available
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.3.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA RTX 4000 Ada Generation (arch=8.9)
Driver version                   535.183.01
CUDA_HOME                        None - invalid!
Pillow                           8.2.0
torchvision                      0.18.1 @/home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/sasini/miniconda3/envs/catseg/lib/python3.12/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[08/24 16:24:18] detectron2 INFO: Command line arguments: Namespace(config_file='configs/vitl_336.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='auto', opts=['OUTPUT_DIR', 'output//eval', 'MODEL.SEM_SEG_HEAD.TEST_CLASS_JSON', 'datasets/ade150.json', 'DATASETS.TEST', '("ade20k_150_test_sem_seg",)', 'TEST.SLIDING_WINDOW', 'True', 'MODEL.SEM_SEG_HEAD.POOLING_SIZES', '[1,1]', 'MODEL.WEIGHTS', 'output//model_final.pth', 'MODEL.WEIGHTS', 'checkpoints/model_large.pth'])
[08/24 16:24:18] detectron2 INFO: Contents of args.config_file=configs/vitl_336.yaml:
_BASE_: config.yaml
MODEL:
  META_ARCHITECTURE: "CATSeg"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  SEM_SEG_HEAD:
    NAME: "CATSegHead"
    IN_FEATURES: ["res2", "res3", "res4"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 171
    TRAIN_CLASS_JSON: "datasets/coco.json"
    TEST_CLASS_JSON: "datasets/coco.json"
    CLIP_PRETRAINED: "ViT-L/14@336px"
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    DECODER_DIMS: [64, 32]
    DECODER_GUIDANCE_DIMS: [256, 128]
    DECODER_GUIDANCE_PROJ_DIMS: [32, 16]
    NUM_LAYERS: 2
    NUM_HEADS: 4
    HIDDEN_DIMS: 128
    POOLING_SIZES: [2, 2]
    FEATURE_RESOLUTION: [24, 24]
    WINDOW_SIZES: 12
    ATTENTION_TYPE: "linear"
    CLIP_FINETUNE: "attention"
  PROMPT_ENSEMBLE_TYPE: "single"
INPUT:
  MIN_SIZE_TRAIN: (384, )
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (384, 384)
  SIZE_DIVISIBILITY: 384 
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "mask_former_semantic"
SOLVER:
  IMS_PER_BATCH: 4 
  LR_SCHEDULER_NAME: WarmupCosineLR
  BASE_LR: 0.0002
  MAX_ITER: 80000
  BACKBONE_MULTIPLIER: 0.0
  CLIP_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 5000
  
[08/24 16:24:18] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: true
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ade20k_150_test_sem_seg
  TRAIN:
  - coco_2017_train_stuff_all_sem_seg
  VAL_ALL:
  - coco_2017_val_all_stuff_sem_seg
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 384
    - 384
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 384
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 384
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  CLIP_PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  CLIP_PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.323163
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    SIZE_DIVISIBILITY: 32
  MASK_ON: false
  META_ARCHITECTURE: CATSeg
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROMPT_ENSEMBLE: false
  PROMPT_ENSEMBLE_TYPE: single
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    APPEARANCE_GUIDANCE_DIM: 768
    APPEARANCE_GUIDANCE_PROJ_DIM: 128
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    ATTENTION_TYPE: linear
    CLIP_FINETUNE: attention
    CLIP_PRETRAINED: ViT-L/14@336px
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    DECODER_DIMS:
    - 64
    - 32
    DECODER_GUIDANCE_DIMS:
    - 256
    - 128
    DECODER_GUIDANCE_PROJ_DIMS:
    - 32
    - 16
    FEATURE_RESOLUTION:
    - 24
    - 24
    HIDDEN_DIMS: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: CATSegHead
    NORM: GN
    NUM_CLASSES: 171
    NUM_HEADS: 4
    NUM_LAYERS: 2
    POOLING_SIZES:
    - 1
    - 1
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    PROMPT_DEPTH: 0
    PROMPT_LENGTH: 0
    TEST_CLASS_INDEXES: datasets/coco/coco_stuff/split/unseen_indexes.json
    TEST_CLASS_JSON: datasets/ade150.json
    TEXT_GUIDANCE_DIM: 768
    TEXT_GUIDANCE_PROJ_DIM: 128
    TRAIN_CLASS_INDEXES: datasets/coco/coco_stuff/split/seen_indexes.json
    TRAIN_CLASS_JSON: datasets/coco.json
    USE_DEPTHWISE_SEPARABLE_CONV: false
    WINDOW_SIZES: 12
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  WEIGHTS: checkpoints/model_large.pth
OUTPUT_DIR: output//eval
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BACKBONE_MULTIPLIER: 0.0
  BASE_LR: 0.0002
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 0.01
  GAMMA: 0.1
  IMS_PER_BATCH: 4
  LR_SCHEDULER_NAME: WarmupCosineLR
  MAX_ITER: 80000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
  SLIDING_WINDOW: true
VERSION: 2
VIS_PERIOD: 0

[08/24 16:24:18] detectron2 INFO: Full config saved to output//eval/config.yaml
[08/24 16:24:18] d2.utils.env INFO: Using a generated random seed 18589405
[08/24 16:24:23] d2.engine.defaults INFO: Model:
CATSeg(
  (sem_seg_head): CATSegHead(
    (predictor): CATSegPredictor(
      (clip_model): CLIP(
        (visual): VisualTransformer(
          (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (transformer): Transformer(
            (resblocks): Sequential(
              (0): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): ResidualAttentionBlock(
                (attn): Attention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
                )
                (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): Sequential(
                  (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): Attention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (token_embedding): Embedding(49408, 768)
        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Aggregator(
        (layers): ModuleList(
          (0-1): 2 x AggregatorLayer(
            (swin_block): SwinTransformerBlockWrapper(
              (block_1): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (block_2): SwinTransformerBlock(
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  (q): Linear(in_features=256, out_features=128, bias=True)
                  (k): Linear(in_features=256, out_features=128, bias=True)
                  (v): Linear(in_features=128, out_features=128, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (drop1): Dropout(p=0.0, inplace=False)
                  (norm): Identity()
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (drop2): Dropout(p=0.0, inplace=False)
                )
              )
              (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
            (attention): ClassTransformerLayer(
              (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
              (attention): AttentionLayer(
                (q): Linear(in_features=256, out_features=128, bias=True)
                (k): Linear(in_features=256, out_features=128, bias=True)
                (v): Linear(in_features=128, out_features=128, bias=True)
                (attention): LinearAttention()
              )
              (MLP): Sequential(
                (0): Linear(in_features=128, out_features=512, bias=True)
                (1): ReLU()
                (2): Linear(in_features=512, out_features=128, bias=True)
              )
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (guidance_projection): Sequential(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU()
        )
        (text_guidance_projection): Sequential(
          (0): Linear(in_features=768, out_features=128, bias=True)
          (1): ReLU()
        )
        (decoder_guidance_projection): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU()
          )
        )
        (decoder1): Up(
          (up): ConvTranspose2d(128, 96, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(4, 64, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(4, 64, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (decoder2): Up(
          (up): ConvTranspose2d(64, 48, kernel_size=(2, 2), stride=(2, 2))
          (conv): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): GroupNorm(2, 32, eps=1e-05, affine=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): GroupNorm(2, 32, eps=1e-05, affine=True)
              (5): ReLU(inplace=True)
            )
          )
        )
        (head): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (upsample1): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))
  (upsample2): ConvTranspose2d(1024, 128, kernel_size=(4, 4), stride=(4, 4))
)
[08/24 16:24:23] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:24:23] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/model_large.pth ...
[08/24 16:24:24] d2.data.datasets.coco INFO: Loaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation
[08/24 16:24:24] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]
[08/24 16:24:24] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[08/24 16:24:24] d2.data.common INFO: Serializing 2000 elements to byte tensors and concatenating them all ...
[08/24 16:24:24] d2.data.common INFO: Serialized dataset takes 0.39 MiB
[08/24 16:24:24] d2.data.datasets.coco INFO: Loaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation
[08/24 16:24:24] d2.evaluation.evaluator INFO: Start inference on 2000 batches
[08/24 16:24:34] d2.evaluation.evaluator INFO: Inference done 11/2000. Dataloading: 0.0007 s/iter. Inference: 0.8107 s/iter. Eval: 0.0073 s/iter. Total: 0.8188 s/iter. ETA=0:27:08
